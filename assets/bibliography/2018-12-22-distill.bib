@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}
@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{Yan22,
	abstract = {Information Extraction (IE) in Natural Language Processing (NLP) aims to extract structured information from unstructured text to assist a computer in understanding natural language. Machine learning-based IE methods bring more intelligence and possibilities but require an extensive and accurate labeled corpus. In the materials science domain, giving reliable labels is a laborious task that requires the efforts of many professionals. To reduce manual intervention and automatically generate materials corpus during IE, in this work, we propose a semi-supervised IE framework for materials via automatically generated corpus. Taking the superalloy data extraction in our previous work as an example, the proposed framework using Snorkel automatically labels the corpus containing property values. Then Ordered Neurons-Long Short-Term Memory (ON-LSTM) network is adopted to train an information extraction model on the generated corpus. The experimental results show that the F1-score of Œ≥'solvus temperature, density and solidus temperature of superalloys are 83.90{\%}, 94.02{\%}, 89.27{\%}, respectively. Furthermore, we conduct similar experiments on other materials, the experimental results show that the proposed framework is universal in the field of materials.},
	author = {Yan, Rongen and Jiang, Xue and Wang, Weiren and Dang, Depeng and Su, Yanjing},
	da = {2022/07/13},
	date-added = {2023-01-12 11:41:08 +0400},
	date-modified = {2023-01-12 11:41:08 +0400},
	doi = {10.1038/s41597-022-01492-2},
	id = {Yan2022},
	isbn = {2052-4463},
	journal = {Scientific Data},
	number = {1},
	pages = {401},
	title = {Materials information extraction via automatically generated corpus},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41597-022-01492-2},
	volume = {9},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41597-022-01492-2}}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge‚ÄìKutta methods, Nonlinear dynamics}
}
@article{zaki22,
title = {Natural language processing-guided meta-analysis and structure factor database extraction from glass literature},
journal = {Journal of Non-Crystalline Solids: X},
volume = {15},
pages = {100103},
year = {2022},
issn = {2590-1591},
doi = {https://doi.org/10.1016/j.nocx.2022.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2590159122000231},
author = {Mohd Zaki and Sahith Reddy Namireddy and Tanu Pittie and Vaibhav Bihani and Shweta Rani Keshri and Vineeth Venugopal and Nitya Nand Gosvami and  Jayadeva and N.M. Anoop Krishnan},
keywords = {Glasses, Structure factor, Natural language processing}
}
@inproceedings{ansley22,
author = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
title = {The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming},
year = {2022},
isbn = {9781450396431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511861.3511863},
doi = {10.1145/3511861.3511863},
abstract = {Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI‚Äôs GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known ‚ÄúRainfall Problem‚Äù along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.},
booktitle = {Australasian Computing Education Conference},
pages = {10‚Äì19},
numpages = {10},
keywords = {AI, neural networks, copilot, machine learning, CS1, Codex, deep learning, novice programming, artificial intelligence, GPT-3, introductory programming, OpenAI, academic integrity, GitHub, code writing, code generation},
location = {Virtual Event, Australia},
series = {ACE '22}
}

